[
  {
    "objectID": "posts/2020_10_06_double_descent/index.html",
    "href": "posts/2020_10_06_double_descent/index.html",
    "title": "On the Universality of the Double Descent Peak in Ridgeless Regression",
    "section": "",
    "text": "I have just uploaded a new paper (https://arxiv.org/abs/2010.01851) in which I prove that a certain class of linear regression methods always performs badly on noisy data when the number of model parameters is close to the number of data points. This blog post is not a summary of the paper. Instead, I want to provide some intuition with simple examples and show some aspects of the proof strategy in a simplified setting. After this simplified analysis, I will show a proof that I find quite amusing, in particular because it involves computing the expected determinant of certain random matrices using combinatorics. Unfortunately, this proof did not make it to the paper since it gives a slightly worse result than the proof that made it to the paper."
  },
  {
    "objectID": "posts/2020_10_06_double_descent/index.html#interpolating-methods-and-double-descent",
    "href": "posts/2020_10_06_double_descent/index.html#interpolating-methods-and-double-descent",
    "title": "On the Universality of the Double Descent Peak in Ridgeless Regression",
    "section": "Interpolating Methods and Double Descent",
    "text": "Interpolating Methods and Double Descent\nSuppose that we have a regression problem of the following (typical) form: Given \\(n \\geq 1\\) samples \\((\\boldsymbol{x}_1, y_1), \\ldots, (\\boldsymbol{x}_n, y_n)\\) with \\(\\boldsymbol{x}_i \\in \\mathbb{R}^d\\) and \\(y_i \\in \\mathbb{R}\\), find a function \\(f: \\mathbb{R}^d \\to \\mathbb{R}\\) that “fits the data well”. For simplicity, we will consider the model where \\(\\boldsymbol{x}_1, \\ldots, \\boldsymbol{x}_n\\) are drawn independently from an unknown distribution \\(P_X\\) and \\(y_i = f^*(\\boldsymbol{x}_i) + \\varepsilon_i\\), where \\(f^*: \\mathbb{R}^d \\to \\mathbb{R}\\) is the unknown target function and \\(\\varepsilon_i\\) are independent standard normal noise variables. We define the error made by our learned function as\n\\[\\mathcal{E}(f) :=\\mathbb{E}_{\\boldsymbol{x}\\sim P_X} (f(\\boldsymbol{x}) - f^*(\\boldsymbol{x}))^2~.\\]\nAlthough we cannot exactly compute \\(\\mathcal{E}(f)\\) since \\(P_X\\) and \\(f^*\\) are unknown, we want \\(\\mathcal{E}(f)\\) to be low such that we make good predictions on test samples \\(\\boldsymbol{x}\\) that are drawn from the same distribution \\(P_X\\) as the training samples \\(\\boldsymbol{x}_1, \\ldots, \\boldsymbol{x}_n\\). Since our labels \\(y_i\\) are noisy due to the corruption by \\(\\varepsilon_i\\), conventional wisdom tells us that if \\(f\\) interpolates the samples, it would be overfitting and therefore a bad estimate. However, interpolating the data can still give small \\(\\mathcal{E}(f)\\), even with label noise:\n\nAlthough putting small spikes around the data points may look silly in this example with \\(d=1\\), it would look much more natural in high dimensions \\(d\\) where the samples \\(\\boldsymbol{x}_i\\) can all have large distance from each other and the spikes can be much wider without affecting \\(\\mathcal{E}(f)\\) much. Indeed, modern neural networks can often achieve good generalization performance despite being so large that they can interpolate the data.\nResearch around the “Double Descent” phenomenon has found that in various settings with few to no regularization, ML methods whose number of parameters \\(p\\) is close to the number of samples \\(n\\) perform rather badly. For example, common linear models with \\(p=n\\) are barely able to interpolate the data, but there is only one interpolating solution, and this solution can be quite wiggly:\n\nIn the underparameterized case (\\(p \\ll n\\)), the linear model is not able to interpolate the data and the noise partially cancels out:\n\nIn the overparameterized case (\\(p \\gg n\\)), there are multiple interpolating solutions. The linear model chooses the one with the smallest parameter norm, and this choice may provide less overshoots than in the case \\(p = n\\):\n\nIn the paper, I show that a large class of linear models performs badly for \\(p \\approx n\\) when the labels \\(y_i\\) are noisy. The considered linear models are defined in the next section."
  },
  {
    "objectID": "posts/2020_10_06_double_descent/index.html#linear-regression-with-features-and-a-lower-bound",
    "href": "posts/2020_10_06_double_descent/index.html#linear-regression-with-features-and-a-lower-bound",
    "title": "On the Universality of the Double Descent Peak in Ridgeless Regression",
    "section": "Linear Regression with Features and a Lower Bound",
    "text": "Linear Regression with Features and a Lower Bound\nWe consider unregularized linear regression with features, where we choose a fixed feature map \\(\\phi: \\mathbb{R}^d \\to \\mathbb{R}^p\\) for some number of parameters \\(p\\) and then learn the parameters \\(\\boldsymbol{\\beta}\\in \\mathbb{R}^p\\) for the function \\(f(\\boldsymbol{x}) = \\boldsymbol{\\beta}^\\top \\phi(\\boldsymbol{x})\\). The parameters \\(\\boldsymbol{\\beta}\\) are chosen to minimize the training error\n\\[\\frac{1}{n} \\sum_{i=1}^n (f(\\boldsymbol{x}_i) - y_i)^2~.\\]\nIn case that there are multiple optimal parameters \\(\\boldsymbol{\\beta}\\), the one with minimum Euclidean norm is chosen. This procedure can be expressed as an equation: \\(\\boldsymbol{\\beta}= \\boldsymbol{Z}^+ \\boldsymbol{y}\\), where \\(\\boldsymbol{Z}^+\\) is the Moore-Penrose pseudoinverse of\n\\[\\boldsymbol{Z}= \\begin{pmatrix}\n\\phi(\\boldsymbol{x}_1)^\\top \\\\\n\\vdots \\\\\n\\phi(\\boldsymbol{x}_n)^\\top\n\\end{pmatrix}~.\\]\nThe function \\(f\\) found in this way is random, since it depends on the random draw of the training data. We can therefore define the expected error over all random draws of training data sets:\n\\[\\mathcal{E}:=\\mathbb{E}_f \\mathcal{E}(f)~.\\]\nSuppose that the linear model can interpolate \\(p\\) randomly drawn data samples with probability one and that some other weak assumptions are satisfied. I show in the paper that many feature maps \\(\\phi\\) and input distributions \\(P_X\\) satisfy these assumptions. In my paper, I prove that under these assumptions, the following holds:\n\nIf \\(p \\geq n\\), then \\(\\mathcal{E}\\geq \\frac{n}{p+1-n}\\).\nIf \\(p &lt; n\\), then \\(\\mathcal{E}\\geq \\frac{p}{n+1-p}\\).\n\nIf the noise \\(\\varepsilon_i\\) has a variance other than \\(1\\), the lower bounds are scaled proportionally to the variance."
  },
  {
    "objectID": "posts/2020_10_06_double_descent/index.html#a-very-simple-case",
    "href": "posts/2020_10_06_double_descent/index.html#a-very-simple-case",
    "title": "On the Universality of the Double Descent Peak in Ridgeless Regression",
    "section": "A very simple case",
    "text": "A very simple case\nHere, I want to illustrate some of the main arguments for my lower bound in the simplest case with \\(n = p = 1\\), i.e. we have one sample \\((\\boldsymbol{x}_1, y_1)\\) with \\(\\phi(\\boldsymbol{x}_i) \\in \\mathbb{R}\\). Of course, nobody would expect to get good results with one sample in regression, so this case is not particularly interesting from a practical perspective, but maybe it is at least possible to pick a “lucky” feature map which gives good estimates for one particular target function \\(f^*\\)?\nIt can be shown that for linear models, the zero target function \\(f^* \\equiv 0\\) yields the smallest possible \\(\\mathcal{E}\\) among all possible target functions. Therefore, we will assume \\(f^* \\equiv 0\\) in the following.\nThe regression function is \\(f(\\boldsymbol{x}) = \\beta \\phi(\\boldsymbol{x})\\). In order to interpolate the single datapoint \\((\\boldsymbol{x}_1, y_1)\\), the parameter \\(\\beta\\) must satisfy \\(y_1 = \\beta \\phi(\\boldsymbol{x}_1)\\), or \\(\\beta = \\frac{y_1}{\\phi(\\boldsymbol{x}_1)}\\). Of course, this is only possible if \\(\\phi(\\boldsymbol{x}_1)\\) is nonzero. We therefore assume that \\(\\phi(\\boldsymbol{x}_1)\\) is nonzero almost surely, that is, \\(\\phi(\\boldsymbol{x}_1) \\neq 0\\) with probability \\(1\\). For this estimate, the corresponding error is\n\\[\\mathcal{E}(f) = \\mathbb{E}_{\\boldsymbol{x}\\sim P_X} (f(\\boldsymbol{x}) - f^*(\\boldsymbol{x}))^2 = \\mathbb{E}_{\\boldsymbol{x}\\sim P_X} \\beta^2 \\phi(\\boldsymbol{x})^2 = \\frac{y_1^2}{\\phi(\\boldsymbol{x}_1)^2} \\mathbb{E}_{\\boldsymbol{x}\\sim P_X} \\phi(\\boldsymbol{x})^2~.\n\\]\nSince \\(f^* \\equiv 0\\), \\(y_1 = \\varepsilon_1\\) and \\(\\varepsilon_1\\) is independent of \\(\\boldsymbol{x}_1\\) by assumption. We also assumed that \\(\\varepsilon_1\\) is standard normal, hence \\(\\mathbb{E}y_1^2 = \\mathbb{E}\\varepsilon_1^2 = \\operatorname{Var}(\\varepsilon_1) = 1\\). Therefore,\n\\[\\begin{aligned}\n\\mathcal{E}&= \\mathbb{E}_f \\mathcal{E}(f) = \\left(\\mathbb{E}y_1^2\\right) \\cdot \\left(\\mathbb{E}_{\\boldsymbol{x}_1 \\sim P_X} \\frac{1}{\\phi(\\boldsymbol{x}_1)^2}\\right) \\cdot \\left(\\mathbb{E}_{\\boldsymbol{x}\\sim P_X} \\phi(\\boldsymbol{x})^2\\right) \\\\\n&= \\left(\\mathbb{E}_{\\boldsymbol{x}_1 \\sim P_X} \\frac{1}{\\phi(\\boldsymbol{x}_1)^2}\\right) \\cdot \\left(\\mathbb{E}_{\\boldsymbol{x}\\sim P_X} \\phi(\\boldsymbol{x})^2\\right)~.\\end{aligned}\\]\nA very simple feature map is the constant feature map with \\(\\phi(\\boldsymbol{x}) = 1\\) for all \\(\\boldsymbol{x}\\). For this feature map, we immediately obtain\n\\[\\mathcal{E}= 1 \\cdot 1 = 1~.\\]\nFor the constant feature map, our interpolating function \\(f(x)\\) is always constant. In an attempt to decrease \\(\\mathcal{E}\\), one might try to bring the feature map closer to zero away from the data point \\(\\boldsymbol{x}_1\\). For example, choosing the feature map \\(\\phi(x) = e^{-x^2}\\) can reduce \\(\\mathcal{E}(f)\\) if \\(x_1 = 0\\):\n\nThe downside is that in our model, the feature map \\(\\phi\\) is constant and chosen before seeing \\(\\boldsymbol{x}_1\\). Therefore, if \\(\\boldsymbol{x}_1\\) lies somewhere else, the parameter \\(\\beta\\) might need to be much larger, which can drastically increase \\(\\mathcal{E}(f)\\):\n\nAs another example, assume that \\(P_X\\) is the uniform distribution on \\([0, 1]\\) and our feature map is \\(\\phi(x) = e^x\\). Then,\n\\[\\begin{aligned}\n\\mathbb{E}_{x \\sim P_X} \\phi(x)^2 &= \\int_0^1 e^{2x} \\,\\mathrm{d}x = \\frac{1}{2}(e^2-1) \\\\\n\\mathbb{E}_{x_1 \\sim P_X} \\frac{1}{\\phi(x_1)^2} &= \\int_0^1 e^{-2x} \\,\\mathrm{d}x = \\frac{1}{2}(1 - e^{-2})\\end{aligned}\n\\]\nand since \\(f^* = 0\\), \\(y_1 = \\varepsilon_1\\) is independent of \\(x_1\\) with \\(\\mathbb{E}y_1^2 = 1\\). Thus,\n\\[\\mathcal{E}= \\left(\\mathbb{E}_{x_1 \\sim P_X} \\frac{1}{\\phi(x_1)^2}\\right) \\cdot \\left(\\mathbb{E}_{\\boldsymbol{x}\\sim P_X} \\phi(\\boldsymbol{x})^2\\right) = \\frac{1}{4} (e^2 - 1)(1 - e^{-2}) \\approx 1.38~.\\]\nWe have seen that for the exponential feature map, \\(\\mathcal{E}\\) was larger than for the constant feature map, at least for this particular input distribution \\(P_X\\). We can use Jensen’s inequality to show that for \\(p=n=1\\), no feature map \\(\\phi\\) yields a lower \\(\\mathcal{E}\\) than the constant feature map, as long as \\(\\phi(\\boldsymbol{x}_1)\\) is nonzero almost surely: Consider the function \\(h: (0, \\infty) \\to (0, \\infty), u \\mapsto 1/u\\). This function is convex because its second derivative \\(h^{(2)}(u) = 2u^{-3}\\) is positive for all \\(u \\in (0, \\infty)\\). But for convex functions \\(h\\), Jensen’s inequality tells us that for any random variable \\(U\\), we have\n\\[\\mathbb{E}h(U) \\geq h(\\mathbb{E}U)~.\\]\nNow, pick the random variable \\(U = \\phi(\\boldsymbol{x}_1)^2\\). By our assumption, \\(U \\in (0, \\infty)\\) holds with probability \\(1\\), hence \\(U\\) is in the domain of our function \\(h\\) with probability one. Jensen’s inequality then yields\n\\[\\mathbb{E}_{\\boldsymbol{x}_1 \\sim P_X} \\frac{1}{\\phi(\\boldsymbol{x}_1)^2} = \\mathbb{E}h(U) \\geq h(\\mathbb{E}U) = \\frac{1}{\\mathbb{E}_{\\boldsymbol{x}_1 \\sim P_X} \\phi(\\boldsymbol{x}_1)^2} = \\frac{1}{\\mathbb{E}_{\\boldsymbol{x}\\sim P_X} \\phi(\\boldsymbol{x})^2}~.\\]\nBut then,\n\\[\\begin{aligned}\n\\mathcal{E}& = \\left(\\mathbb{E}_{\\boldsymbol{x}_1 \\sim P_X} \\frac{1}{\\phi(\\boldsymbol{x}_1)^2}\\right) \\cdot \\left(\\mathbb{E}_{\\boldsymbol{x}\\sim P_X} \\phi(\\boldsymbol{x})^2\\right) \\geq \\frac{1}{\\mathbb{E}_{\\boldsymbol{x}\\sim P_X} \\phi(\\boldsymbol{x})^2} \\cdot \\mathbb{E}_{\\boldsymbol{x}\\sim P_X} \\phi(\\boldsymbol{x})^2 = 1~.\\end{aligned}\\]"
  },
  {
    "objectID": "posts/2020_10_06_double_descent/index.html#a-curious-proof",
    "href": "posts/2020_10_06_double_descent/index.html#a-curious-proof",
    "title": "On the Universality of the Double Descent Peak in Ridgeless Regression",
    "section": "A curious proof",
    "text": "A curious proof\nThe generalization of the argument in the last section to general \\(n, p \\geq 1\\) involves independent random vectors \\(\\boldsymbol{w}_1, \\ldots, \\boldsymbol{w}_n\\) and the matrix\n\\[\\boldsymbol{W}:=\\begin{pmatrix}\n\\boldsymbol{w}_1^\\top \\\\\n\\vdots \\\\\n\\boldsymbol{w}_n^\\top\n\\end{pmatrix}~.\\]\nIn the case \\(p = 1\\), we have \\(w_i = \\frac{\\phi(\\boldsymbol{x}\\_i)}{\\sqrt{\\mathbb{E}\\_{\\boldsymbol{x}\\sim P_X} \\phi(\\boldsymbol{x})^2}}\\), which yields \\(\\mathbb{E}w_i^2 = 1\\). The analogous definition for \\(p \\geq 1\\) can be found in the paper and yields \\(\\mathbb{E}\\boldsymbol{w}_i \\boldsymbol{w}_i^\\top = \\boldsymbol{I}_p\\), where \\(\\boldsymbol{I}_p \\in \\mathbb{R}^{p \\times p}\\) is the identity matrix. In the overparameterized case \\(p \\geq n\\), it is shown in the paper that\n\\[\\mathcal{E}\\geq \\mathbb{E}\\operatorname{tr}((\\boldsymbol{W}\\boldsymbol{W}^\\top)^{-1})~,\\]\nwhere \\(\\boldsymbol{W}\\boldsymbol{W}^\\top\\) is invertible with probability one under the given assumptions in the paper. Step 3.1 in the proof of Theorem 3 in Appendix F in the paper then shows that\n\\[\\mathbb{E}\\operatorname{tr}((\\boldsymbol{W}\\boldsymbol{W}^\\top)^{-1}) \\geq \\frac{n}{p+1-n} = \\frac{n}{p-(n-1)}~.\\]\nIn the following, I will present a different proof showing the weaker lower bound\n\\[\\mathbb{E}\\operatorname{tr}((\\boldsymbol{W}\\boldsymbol{W}^\\top)^{-1}) \\geq \\frac{n}{p-\\frac{n-1}{2}}\\]\nbut with an interesting argument that involves exactly computing \\(\\mathbb{E}\\det(\\boldsymbol{W}\\boldsymbol{W}^\\top)\\) using combinatorics.\nStep 1: Arithmetic-Geometric Mean Inequality. Let \\(a_1, \\ldots, a_n &gt; 0\\). The function \\(h = -\\log\\) is convex and Jensen’s inequality yields\n\\[\\frac{1}{n} \\sum_{i=1}^n -\\log(a_i) \\geq -\\log\\left(\\frac{1}{n} \\sum_{i=1}^n a_i\\right)~.\\]\nApplying the decreasing function \\(g(x) = e^{-x}\\) to both sides yields the well-known inequality between arithmetic and geometric mean:\n\\[\\left(\\prod_{i=1}^n a_i\\right)^{1/n} \\leq \\frac{1}{n} \\sum_{i=1}^n a_i~.\\]\nStep 2: Jensen again. For a matrix \\(\\boldsymbol{A}\\in \\mathbb{R}^{n \\times n}\\), let \\(\\lambda_1(\\boldsymbol{A}), \\ldots, \\lambda_n(\\boldsymbol{A})\\) be the eigenvalues of \\(\\boldsymbol{A}\\). It is well-known that the trace satisfies \\(\\operatorname{tr}(\\boldsymbol{A}) = \\lambda_1(\\boldsymbol{A}) + \\ldots + \\lambda_n(\\boldsymbol{A})\\) and the determinant satisfies \\(\\det(\\boldsymbol{A}) = \\lambda_1(\\boldsymbol{A}) \\cdot \\ldots \\cdot \\lambda_n(\\boldsymbol{A})\\). We know that \\(\\boldsymbol{W}\\boldsymbol{W}^\\top\\) is invertible almost surely and therefore also positive definite, hence the eigenvalues of \\(\\boldsymbol{W}\\boldsymbol{W}^\\top\\) are positive. We can apply the arithmetic-geometric mean inequality to obtain\n\\[\\begin{aligned}\n\\operatorname{tr}((\\boldsymbol{W}\\boldsymbol{W}^\\top)^{-1}) & = n\\cdot \\frac{1}{n} \\sum_{i=1}^n \\lambda_{i}((\\boldsymbol{W}\\boldsymbol{W}^\\top)^{-1}) \\\\\n& \\geq n \\left(\\prod_{i=1}^n \\lambda_{i}((\\boldsymbol{W}\\boldsymbol{W}^\\top)^{-1})\\right)^{1/n} \\\\\n& = n \\left(\\det((\\boldsymbol{W}\\boldsymbol{W}^\\top)^{-1})\\right)^{1/n} = n \\left(\\det(\\boldsymbol{W}\\boldsymbol{W}^\\top)\\right)^{-1/n} \\\\\\end{aligned} \\]\nUsing Jensen’s inequality with the convex function \\(h: (0, \\infty) \\to (0, \\infty), u \\mapsto u^{-1/n}\\), we obtain\n\\[\\mathbb{E}\\operatorname{tr}((\\boldsymbol{W}\\boldsymbol{W}^\\top)^{-1}) \\geq n \\mathbb{E}\\left(\\det(\\boldsymbol{W}\\boldsymbol{W}^\\top)\\right)^{-1/n} \\geq n \\left(\\mathbb{E}\\det(\\boldsymbol{W}\\boldsymbol{W}^\\top)\\right)^{-1/n}~.\\]\nStep 3: Computing the expected determinant. We will compute the determinant using the Leibniz formula, which involves permutations on the set \\(\\{1, \\ldots, n\\}\\). The group of all such permutations is denoted by \\(S_n\\). Each permutation \\(\\pi \\in S_n\\) has an associated signum \\(\\operatorname{sgn}(\\pi) \\in \\{-1, 1\\}\\). The Leibniz formula then states\n\\[\\det(\\boldsymbol{W}\\boldsymbol{W}^\\top) = \\sum_{\\pi \\in S_n} \\operatorname{sgn}(\\pi) \\prod_{i=1}^n (\\boldsymbol{W}\\boldsymbol{W}^\\top)_{i, \\pi(i)} = \\sum_{\\pi \\in S_n} \\operatorname{sgn}(\\pi) \\prod_{i=1}^n \\boldsymbol{w}_i^\\top \\boldsymbol{w}_{\\pi(i)}~.\\]\nNow, the expectation over \\(\\boldsymbol{W}\\) can be interchanged with the sum on the right-hand side, but not with the product, since the terms in the product are not independent. In order to break the product into independent components, we need to take the structure of the permutation \\(\\pi\\) into account. Each permutation \\(\\pi \\in S_n\\) can be broken down into a set of disjoint cycles \\(\\mathcal{C}(\\pi)\\) such that \\(\\pi\\) is the product of its cycles. Consider for example the case where \\(n = 6\\), \\(\\pi(1) = 2, \\pi(2) = 4, \\pi(3) = 6, \\pi(4) = 1, \\pi(5) = 5\\) and \\(\\pi(6) = 3\\). We would then write \\(\\mathcal{C}(\\pi) = \\{(124), (36), (5)\\}\\). In our example, arranging our product according to the cycles yields\n\\[\\begin{aligned}\n\\prod_{i=1}^6 \\boldsymbol{w}_i^\\top \\boldsymbol{w}_{\\pi(i)} &= \\boldsymbol{w}_1^\\top \\boldsymbol{w}_2 \\boldsymbol{w}_2^\\top \\boldsymbol{w}_4 \\boldsymbol{w}_4^\\top \\boldsymbol{w}_1 \\cdot \\boldsymbol{w}_3^\\top \\boldsymbol{w}_6 \\boldsymbol{w}_6^\\top \\boldsymbol{w}_3 \\cdot \\boldsymbol{w}_5^\\top \\boldsymbol{w}_5\\end{aligned}\\]\nWe now perform the following rearrangement, which can be applied to all of the three cycles:\n\\[\\begin{aligned}\n\\boldsymbol{w}_3^\\top \\boldsymbol{w}_6 \\boldsymbol{w}_6^\\top \\boldsymbol{w}_3 = \\operatorname{tr}(\\boldsymbol{w}_3^\\top \\boldsymbol{w}_6 \\boldsymbol{w}_6^\\top \\boldsymbol{w}_3) = \\operatorname{tr}(\\boldsymbol{w}_3 \\boldsymbol{w}_3^\\top \\boldsymbol{w}_6 \\boldsymbol{w}_6^\\top)\\end{aligned}\\]\nThese arguments apply for general \\(n\\) and \\(\\pi\\), yielding the identity\n\\[\\det(\\boldsymbol{W}\\boldsymbol{W}^\\top) = \\sum_{\\pi \\in S_n} \\operatorname{sgn}(\\pi) \\prod_{(j_1j_2\\ldots j_l) \\in \\mathcal{C}(\\pi)} \\operatorname{tr}\\left((\\boldsymbol{w}_{j_1} \\boldsymbol{w}_{j_1}^\\top) (\\boldsymbol{w}_{j_2} \\boldsymbol{w}_{j_2}^\\top) \\ldots (\\boldsymbol{w}_{j_l} \\boldsymbol{w}_{j_l}^\\top)\\right)~.\\]\nNow, the products \\((\\boldsymbol{w}_{j_i} \\boldsymbol{w}_{j_i}^\\top)\\) are actually independent and we can pull in the expected value:\n\\[\\begin{aligned}\n\\mathbb{E}\\det(\\boldsymbol{W}\\boldsymbol{W}^\\top) &= \\sum_{\\pi \\in S_n} \\operatorname{sgn}(\\pi) \\prod_{(j_1j_2\\ldots j_l) \\in \\mathcal{C}(\\pi)} \\operatorname{tr}\\left((\\mathbb{E}\\boldsymbol{w}_{j_1} \\boldsymbol{w}_{j_1}^\\top) (\\mathbb{E}\\boldsymbol{w}_{j_2} \\boldsymbol{w}_{j_2}^\\top) \\ldots (\\mathbb{E}\\boldsymbol{w}_{j_l} \\boldsymbol{w}_{j_l}^\\top)\\right) \\\\\n&= \\sum_{\\pi \\in S_n} \\operatorname{sgn}(\\pi) \\prod_{(j_1j_2\\ldots j_l) \\in \\mathcal{C}(\\pi)} \\operatorname{tr}(\\boldsymbol{I}_p \\cdot \\boldsymbol{I}_p \\cdot \\ldots \\cdot \\boldsymbol{I}_p) \\\\\n&= \\sum_{\\pi \\in S_n} \\operatorname{sgn}(\\pi) \\prod_{(j_1j_2\\ldots j_l) \\in \\mathcal{C}(\\pi)} p \\\\\n&= \\sum_{\\pi \\in S_n} \\operatorname{sgn}(\\pi) p^{|\\mathcal{C}(\\pi)|}~.\\end{aligned}\\]\nIn order to calculate \\(\\operatorname{sgn}(\\pi)\\), we use the known identities\n\\[\\begin{aligned}\n\\operatorname{sgn}(\\pi) & = \\prod_{(j_1 j_2 \\ldots j_l) \\in \\mathcal{C}(\\pi)} \\operatorname{sgn}((j_1 j_2 \\ldots j_l)) = \\prod_{(j_1 j_2 \\ldots j_l) \\in \\mathcal{C}(\\pi)} (-1)^{l+1} = (-1)^{n+|\\mathcal{C}(\\pi)|}~,\\end{aligned}\\]\nwhere we have used in the last equation that the cycle lengths \\(l\\) add up to \\(n\\). At this point, it is helpful to use the following combinatoric fact: The unsigned Stirling numbers of the first kind\n\\[\\begin{aligned}\n\\begin{bmatrix} n \\\\ k \\end{bmatrix} & = |\\{\\pi \\in S_n : |\\mathcal{C}(\\pi)| = k\\}|\\end{aligned}\n\\]\nfor \\(n &gt; 0, k \\geq 0\\) satisfy the identity\n\\[\\begin{aligned}\n\\sum_{k=0}^n \\begin{bmatrix} n \\\\ k \\end{bmatrix} x^k = x(x+1)\\cdot \\ldots \\cdot (x+n-1)\\end{aligned}\n\\]\nfor all \\(x \\in \\mathbb{R}\\).\nWith these considerations, we can compute our expected determinant as\n\\[\\begin{aligned}\n\\mathbb{E}\\det(\\boldsymbol{W}\\boldsymbol{W}^\\top) &= (-1)^n \\sum_{\\pi \\in S_n} (-p)^{|\\mathcal{C}(\\pi)|} = (-1)^n \\sum_{k=0}^n \\begin{bmatrix} n \\\\ k \\end{bmatrix} \\cdot (-p)^k \\\\\n&= (-1)^n (-p)(-p+1) \\cdot \\ldots \\cdot (-p+n-1) \\\\\n&= p(p-1) \\cdot \\ldots \\cdot (p-(n-1))~,\\end{aligned}\n\\]\nand another application of the arithmetic-geometric mean inequality from Step 1 yields\n\\[\\begin{aligned}\n(\\mathbb{E}\\det(\\boldsymbol{W}\\boldsymbol{W}^\\top))^{1/n} &= \\left(p(p-1) \\cdot \\ldots \\cdot (p-(n-1))\\right)^{1/n} \\\\\n&\\leq \\frac{1}{n} (p + (p-1) + \\ldots + (p-(n-1))) \\\\\n&= p - \\frac{n-1}{2}~.\\end{aligned}\\]\nPutting this back into the equation from Step 2, we obtain\n\\[\\begin{aligned}\n\\mathbb{E}\\operatorname{tr}((\\boldsymbol{W}\\boldsymbol{W}^\\top)^{-1}) & \\geq n \\left(\\mathbb{E}\\det(\\boldsymbol{W}\\boldsymbol{W}^\\top)\\right)^{-1/n} \\geq \\frac{n}{p - \\frac{n-1}{2}}~.\\end{aligned}\n\\]"
  },
  {
    "objectID": "demo_posts/welcome/index.html",
    "href": "demo_posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "On the Universality of the Double Descent Peak in Ridgeless Regression\n\n\n\n\n\n\ntheory\n\n\n\n\n\n\n\n\n\nOct 6, 2020\n\n\nDavid Holzmüller\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "demo_posts/post-with-code/index.html",
    "href": "demo_posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "David Holzmüller",
    "section": "",
    "text": "I am a postdoc at INRIA Paris, working with Francis Bach and Gaël Varoquaux.\nE-Mail: firstname.lastname@inria.fr (replace ü by u)"
  },
  {
    "objectID": "index.html#research-interests",
    "href": "index.html#research-interests",
    "title": "David Holzmüller",
    "section": "Research interests",
    "text": "Research interests\nI am currently interested in supervised learning, active learning, and uncertainty quantification with neural networks. I am interested in improving them on tabular data, but have also worked on interatomic potentials together with Viktor Zaverkin and am generally interested in AI4Science. Previously, I have also worked on theory of neural networks (neural tangent kernels, double descent, benign overfitting) and non-log-concave sampling, as well as other topics."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "David Holzmüller",
    "section": "Education",
    "text": "Education\n\nsince 2023: Postdoc at INRIA Paris, co-advised by Francis Bach and Gaël Varoquaux\nApril 2022 - July 2022: Research visit at INRIA Paris, Francis Bach\n2020 - 2023: PhD student at University of Stuttgart, supervised by Ingo Steinwart\n2016 - 2019: M.Sc. Computer Science, University of Stuttgart\n2015 - 2019: B.Sc. Mathematics, University of Stuttgart\n2013 - 2016: B.Sc. Computer Science, University of Stuttgart"
  },
  {
    "objectID": "index.html#papers",
    "href": "index.html#papers",
    "title": "David Holzmüller",
    "section": "Papers",
    "text": "Papers\n\nML for Tabular Data\nDavid Holzmüller, Léo Grinsztajn, and Ingo Steinwart, Better by Default: Strong Pre-Tuned MLPs and Boosted Trees on Tabular Data, Neural Information Processing Systems, 2024. https://arxiv.org/abs/2407.04491\n\n\nActive Learning\nDavid Holzmüller, Viktor Zaverkin, Johannes Kästner, and Ingo Steinwart, A Framework and Benchmark for Deep Batch Active Learning for Regression, Journal of Machine Learning Research, 2023. https://arxiv.org/abs/2203.09410\nDaniel Musekamp, Marimuthu Kalimuthu, David Holzmüller, Makoto Takamoto, Mathias Niepert, Active Learning for Neural PDE Solvers, 2024. https://arxiv.org/abs/2408.01536\nViktor Zaverkin, David Holzmüller, Ingo Steinwart, and Johannes Kästner, Exploring chemical and conformational spaces by batch mode deep active learning, Digital Discovery, 2022. https://doi.org/10.1039/D2DD00034B\nViktor Zaverkin, David Holzmüller, Henrik Christiansen, Federico Errica, Francesco Alesiani, Makoto Takamoto, Mathias Niepert, and Johannes Kästner, Uncertainty-biased molecular dynamics for learning uniformly accurate interatomic potentials, npj Computational Materials, 2024. https://www.nature.com/articles/s41524-024-01254-1\n\n\nNN Theory\nMoritz Haas*, David Holzmüller*, Ulrike von Luxburg, and Ingo Steinwart, Mind the spikes: Benign overfitting of kernels and neural networks in fixed dimension, Neural Information Processing Systems, 2023. https://proceedings.neurips.cc/paper_files/paper/2023/hash/421f83663c02cdaec8c3c38337709989-Abstract-Conference.html\nDavid Holzmüller, On the Universality of the Double Descent Peak in Ridgeless Regression, International Conference on Learning Representations, 2021. https://openreview.net/forum?id=0IO5VdnSAaH\nDavid Holzmüller, Ingo Steinwart, Training Two-Layer ReLU Networks with Gradient Descent is Inconsistent, Journal of Machine Learning Research, 2022. https://jmlr.org/papers/v23/20-830.html\n\n\nSampling Theory\nDavid Holzmüller and Francis Bach, Convergence rates for non-log-concave sampling and log-partition estimation, 2023. https://arxiv.org/abs/2303.03237\n\n\nOther atomistic ML\nViktor Zaverkin*, David Holzmüller*, Ingo Steinwart, and Johannes Kästner, Fast and Sample-Efficient Interatomic Neural Network Potentials for Molecules and Materials Based on Gaussian Moments, J. Chem. Theory Comput. 17, 6658–6670, 2021. https://arxiv.org/abs/2109.09569\nViktor Zaverkin, David Holzmüller, Luca Bonfirraro, and Johannes Kästner, Transfer learning for chemically accurate interatomic neural network potentials, 2022. https://arxiv.org/abs/2212.03916\nViktor Zaverkin, David Holzmüller, Robin Schuldt, and Johannes Kästner, Predicting properties of periodic systems from cluster data: A case study of liquid water, J. Chem. Phys. 156, 114103, 2022. https://aip.scitation.org/doi/full/10.1063/5.0078983\n\n\nOther\nDavid Holzmüller and Dirk Pflüger, Fast Sparse Grid Operations Using the Unidirectional Principle: A Generalized and Unified Framework, 2021. In: Bungartz, HJ., Garcke, J., Pflüger, D. (eds) Sparse Grids and Applications - Munich 2018. Lecture Notes in Computational Science and Engineering, vol 144. Springer, Cham. https://link.springer.com/chapter/10.1007/978-3-030-81362-8_4\nDaniel F. B. Haeufle, Isabell Wochner, David Holzmüller, Danny Driess, Michael Günther, Syn Schmitt, Muscles Reduce Neuronal Information Load: Quantification of Control Effort in Biological vs. Robotic Pointing and Walking, 2020. https://www.frontiersin.org/articles/10.3389/frobt.2020.00077/full\nDavid Holzmüller, Improved Approximation Schemes for the Restricted Shortest Path Problem, 2017. https://arxiv.org/abs/1711.00284\nDavid Holzmüller, Efficient Neighbor-Finding on Space-Filling Curves, 2017. https://arxiv.org/abs/1710.06384"
  },
  {
    "objectID": "index.html#newest-blog-posts",
    "href": "index.html#newest-blog-posts",
    "title": "David Holzmüller",
    "section": "Newest Blog Posts",
    "text": "Newest Blog Posts\n\n\n\n\n\n\n\n\n\n\nOn the Universality of the Double Descent Peak in Ridgeless Regression\n\n\n\nOct 6, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#talk-topics",
    "href": "index.html#talk-topics",
    "title": "David Holzmüller",
    "section": "Talk Topics:",
    "text": "Talk Topics:\n\n(Introduction to) ML for tabular data\nDeep batch active learning for regression\nGeneralization theory of linearized neural networks (UCLA with slides and video, University of Twente, INRIA Paris)"
  },
  {
    "objectID": "index.html#short-cv",
    "href": "index.html#short-cv",
    "title": "David Holzmüller",
    "section": "Short CV",
    "text": "Short CV\n\nsince 2023: Postdoc at INRIA Paris, co-advised by Francis Bach and Gaël Varoquaux\nApril 2022 - July 2022: Research visit at INRIA Paris, Francis Bach\n2020 - 2023: PhD student at University of Stuttgart, supervised by Ingo Steinwart\n2016 - 2019: M.Sc. Computer Science, University of Stuttgart\n2015 - 2019: B.Sc. Mathematics, University of Stuttgart\n2013 - 2016: B.Sc. Computer Science, University of Stuttgart"
  }
]